import asyncio
import ccxt.async_support as ccxt  # ğŸ”¥ æ ¸å¿ƒå‡çº§ï¼šå¼•å…¥ CCXT çš„å¼‚æ­¥å¼•æ“
import pandas as pd
import aiosqlite  # âœ… å‡çº§ï¼šä½¿ç”¨ aiosqlite æ›¿ä»£ sqlite3
import logging
import smtplib
import time
import json
import os
import backoff  # âœ… å‡çº§ï¼šAPI é‡è¯•æœºåˆ¶
from email.mime.text import MIMEText
from email.utils import formataddr
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from typing import Dict, List, Optional
import collections

# ================= âš™ï¸ é…ç½®ç®¡ç† =================
DEFAULT_CONFIG = {
    "email": {
        "sender": "371398370@qq.com",
        "password": "hjqibancxrerbifb",
        "receiver": "371398370@qq.com",
        "smtp_server": "smtp.qq.com",
        "smtp_port": 587
    },
    "scanning": {
        "min_history": 90,
        "max_amplitude": 0.10,
        "squeeze_factor": 0.75,
        "min_volume_usdt": 5000000,
        "radar_interval_seconds": 3600,
        "sniper_interval_seconds": 60,
        "watchlist_expiry_hours": 4,
        "alert_cooldown_hours": 1
    },
    "database": {
        "path": "/root/clawd/scripts/hunter_data.db",
        "log_path": "/root/clawd/scripts/hunter_run.log",
        "pool_size": 5
    },
    "concurrency": {
        "max_concurrent_requests": 10,
        "max_concurrent_analysis": 20,
        "adaptive_enabled": True,
        "adaptive_min_concurrency": 5,
        "adaptive_max_concurrency": 15,
        "adaptive_error_threshold": 0.3,  # 30% é”™è¯¯ç‡è§¦å‘é™çº§
        "adaptive_success_threshold": 0.9,  # 90% æˆåŠŸç‡è§¦å‘å‡çº§
        "adaptive_adjustment_window": 50,  # æœ€è¿‘ 50 æ¬¡è¯·æ±‚ä½œä¸ºè¯„ä¼°çª—å£
        "adaptive_cooldown_seconds": 60  # è°ƒæ•´åå†·å´æ—¶é—´
    },
    "retry": {
        "max_tries": 3,
        "max_time": 60,
        "exponential_base": 2
    },
    "exchanges": {
        "binance": {"enableRateLimit": True, "options": {"defaultType": "future"}, "timeout": 30000},
        "bybit":   {"enableRateLimit": True, "options": {"defaultType": "linear"}, "timeout": 30000},
        "bitget":  {"enableRateLimit": True, "options": {"defaultType": "swap"},   "timeout": 30000},
        "bingx":   {"enableRateLimit": True, "options": {"defaultType": "swap"},   "timeout": 30000},
        "msx":     {"enableRateLimit": True, "options": {"defaultType": "swap"},   "timeout": 30000}
    },
    "logging": {
        "level": "INFO",
        "format": "%(asctime)s [%(levelname)s] %(message)s"
    }
}

def load_config(config_path="/root/clawd/scripts/config.json"):
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤é…ç½®"""
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # åˆå¹¶é…ç½®ï¼ˆé»˜è®¤å€¼ + é…ç½®æ–‡ä»¶ï¼‰
                def deep_merge(default, override):
                    result = default.copy()
                    for key, value in override.items():
                        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                            result[key] = deep_merge(result[key], value)
                        else:
                            result[key] = value
                    return result
                return deep_merge(DEFAULT_CONFIG, config)
        except Exception as e:
            logging.warning(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return DEFAULT_CONFIG
    else:
        logging.info("ğŸ“ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return DEFAULT_CONFIG

# åŠ è½½é…ç½®
CONFIG = load_config()

# ä»é…ç½®ä¸­æå–å¸¸ç”¨å˜é‡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
SENDER_EMAIL = CONFIG["email"]["sender"]
SENDER_PASSWORD = CONFIG["email"]["password"]
RECEIVER_EMAIL = CONFIG["email"]["receiver"]
SMTP_SERVER = CONFIG["email"]["smtp_server"]
SMTP_PORT = CONFIG["email"]["smtp_port"]

MIN_HISTORY = CONFIG["scanning"]["min_history"]
MAX_AMPLITUDE = CONFIG["scanning"]["max_amplitude"]
SQUEEZE_FACTOR = CONFIG["scanning"]["squeeze_factor"]
MIN_VOLUME_USDT = CONFIG["scanning"]["min_volume_usdt"]
RADAR_INTERVAL = CONFIG["scanning"]["radar_interval_seconds"]
SNIPER_INTERVAL = CONFIG["scanning"]["sniper_interval_seconds"]
WATCHLIST_EXPIRY_HOURS = CONFIG["scanning"]["watchlist_expiry_hours"]
ALERT_COOLDOWN_HOURS = CONFIG["scanning"]["alert_cooldown_hours"]

DB_FILE = CONFIG["database"]["path"]
LOG_FILE = CONFIG["database"]["log_path"]
DB_POOL_SIZE = CONFIG["database"].get("pool_size", 5)

MAX_CONCURRENT_REQUESTS = CONFIG["concurrency"]["max_concurrent_requests"]
MAX_CONCURRENT_ANALYSIS = CONFIG["concurrency"]["max_concurrent_analysis"]

RETRY_MAX_TRIES = CONFIG["retry"].get("max_tries", 3)
RETRY_MAX_TIME = CONFIG["retry"].get("max_time", 60)

EXCHANGES_TO_LOAD = CONFIG["exchanges"]

# ================= ğŸ“ ä¸“ä¸šæ—¥å¿—ç³»ç»Ÿ =================
logging.basicConfig(
    level=getattr(logging, CONFIG["logging"]["level"]),
    format=CONFIG["logging"]["format"],
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# ================= ğŸ”„ API é‡è¯•æœºåˆ¶ =================
def should_retry_api_error(e):
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•çš„ API é”™è¯¯"""
    # ç½‘ç»œç›¸å…³é”™è¯¯
    if isinstance(e, (ConnectionError, TimeoutError)):
        return True
    # CCXT ç‰¹å®šé”™è¯¯
    if hasattr(e, '__class__'):
        error_name = e.__class__.__name__
        # ç½‘ç»œè¶…æ—¶ã€é€Ÿç‡é™åˆ¶ã€æœåŠ¡ä¸å¯ç”¨
        retryable_errors = [
            'NetworkError',
            'RequestTimeout',
            'RateLimitExceeded',
            'ExchangeNotAvailable',
            'DDoSProtection'
        ]
        if any(err in error_name for err in retryable_errors):
            return True
    return False

def on_retry(details):
    """é‡è¯•æ—¶çš„å›è°ƒ"""
    logging.warning(
        f"ğŸ”„ API è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•... "
        f"å°è¯• {details['tries']}/{RETRY_MAX_TRIES}, "
        f"ç­‰å¾… {details['wait']:.1f}ç§’, "
        f"é”™è¯¯: {details['exception']}"
    )

# åˆ›å»ºé€šç”¨çš„å¼‚æ­¥é‡è¯•è£…é¥°å™¨
async_retry = backoff.on_exception(
    backoff.expo,
    Exception,
    max_tries=RETRY_MAX_TRIES,
    max_time=RETRY_MAX_TIME,
    giveup=lambda e: not should_retry_api_error(e),
    on_backoff=on_retry
)

# ================= ğŸ§  è‡ªé€‚åº”å¹¶å‘æ§åˆ¶å™¨ =================
class AdaptiveConcurrencyController:
    """è‡ªé€‚åº”å¹¶å‘æ§åˆ¶å™¨ - æ ¹æ®ç½‘ç»œçŠ¶å†µåŠ¨æ€è°ƒæ•´å¹¶å‘æ•°"""
    def __init__(
        self,
        max_concurrency: int,
        min_concurrency: int = 5,
        max_adaptive_limit: int = 15,
        error_threshold: float = 0.3,
        success_threshold: float = 0.9,
        window_size: int = 50,
        cooldown_seconds: int = 60
    ):
        self.max_concurrency = max_concurrency
        self.min_concurrency = min_concurrency
        self.max_adaptive_limit = max_adaptive_limit
        self.error_threshold = error_threshold
        self.success_threshold = success_threshold
        self.window_size = window_size
        self.cooldown_seconds = cooldown_seconds

        # å½“å‰å¹¶å‘æ•°
        self.current_concurrency = max_concurrency

        # è¯·æ±‚å†å²ï¼ˆTrue=æˆåŠŸ, False=å¤±è´¥ï¼‰
        self.request_history: collections.deque = collections.deque(maxlen=window_size)

        # å†·å´æ—¶é—´
        self.last_adjustment_time = 0

        # ä¿¡å·é‡
        self.semaphore: Optional[asyncio.Semaphore] = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0

    async def initialize(self):
        """åˆå§‹åŒ–ä¿¡å·é‡"""
        self.semaphore = asyncio.Semaphore(self.current_concurrency)
        logging.info(f"ğŸ›ï¸ è‡ªé€‚åº”å¹¶å‘æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ (å½“å‰: {self.current_concurrency}, èŒƒå›´: {self.min_concurrency}-{self.max_adaptive_limit})")

    def record_request(self, success: bool):
        """è®°å½•è¯·æ±‚ç»“æœ"""
        self.total_requests += 1
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1

        self.request_history.append(success)

        # å°è¯•è°ƒæ•´å¹¶å‘æ•°
        if len(self.request_history) >= self.window_size:
            self._try_adjust_concurrency()

    def _try_adjust_concurrency(self):
        """å°è¯•è°ƒæ•´å¹¶å‘æ•°"""
        now = time.time()

        # æ£€æŸ¥å†·å´æ—¶é—´
        if now - self.last_adjustment_time < self.cooldown_seconds:
            return

        # è®¡ç®—æˆåŠŸç‡
        success_count = sum(1 for result in self.request_history if result)
        success_rate = success_count / len(self.request_history)

        # é”™è¯¯ç‡è¿‡é«˜ï¼Œé™ä½å¹¶å‘
        if success_rate < (1 - self.error_threshold) and self.current_concurrency > self.min_concurrency:
            self.current_concurrency = max(self.min_concurrency, self.current_concurrency - 1)
            self.last_adjustment_time = now
            logging.warning(
                f"ğŸ“‰ è‡ªé€‚åº”é™çº§: æˆåŠŸç‡ {success_rate:.1%} < {(1-self.error_threshold):.1%}, "
                f"å¹¶å‘æ•°ä» {self.current_concurrency+1} é™è‡³ {self.current_concurrency}"
            )
            self._update_semaphore()

        # æˆåŠŸç‡å¾ˆé«˜ï¼Œå°è¯•æé«˜å¹¶å‘
        elif success_rate >= self.success_threshold and self.current_concurrency < self.max_adaptive_limit:
            self.current_concurrency = min(self.max_adaptive_limit, self.current_concurrency + 1)
            self.last_adjustment_time = now
            logging.info(
                f"ğŸ“ˆ è‡ªé€‚åº”å‡çº§: æˆåŠŸç‡ {success_rate:.1%} >= {self.success_threshold:.1%}, "
                f"å¹¶å‘æ•°ä» {self.current_concurrency-1} å‡è‡³ {self.current_concurrency}"
            )
            self._update_semaphore()

    def _update_semaphore(self):
        """æ›´æ–°ä¿¡å·é‡"""
        if self.semaphore:
            # æ›¿æ¢ä¿¡å·é‡
            self.semaphore = asyncio.Semaphore(self.current_concurrency)

    async def acquire(self):
        """è·å–å¹¶å‘è®¸å¯"""
        if self.semaphore:
            return await self.semaphore.acquire()
        return True

    def release(self):
        """é‡Šæ”¾å¹¶å‘è®¸å¯"""
        if self.semaphore:
            self.semaphore.release()

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        success_count = sum(1 for result in self.request_history if result)
        success_rate = success_count / len(self.request_history) if self.request_history else 0

        return {
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": success_rate,
            "current_concurrency": self.current_concurrency,
            "min_concurrency": self.min_concurrency,
            "max_concurrency": self.max_adaptive_limit
        }

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass

# ================= ğŸŒ å¼‚æ­¥äº¤æ˜“æ‰€æ±  =================
exchanges_dict = {}
watchlist = {}
alert_history = {}

# åˆå§‹åŒ–è‡ªé€‚åº”å¹¶å‘æ§åˆ¶å™¨
adaptive_config = CONFIG["concurrency"]
adaptive_controller = AdaptiveConcurrencyController(
    max_concurrency=MAX_CONCURRENT_REQUESTS,
    min_concurrency=adaptive_config.get("adaptive_min_concurrency", 5),
    max_adaptive_limit=adaptive_config.get("adaptive_max_concurrency", 15),
    error_threshold=adaptive_config.get("adaptive_error_threshold", 0.3),
    success_threshold=adaptive_config.get("adaptive_success_threshold", 0.9),
    window_size=adaptive_config.get("adaptive_adjustment_window", 50),
    cooldown_seconds=adaptive_config.get("adaptive_cooldown_seconds", 60)
)

analysis_semaphore = asyncio.Semaphore(MAX_CONCURRENT_ANALYSIS)

# ================= ğŸ’¾ aiosqlite è¿æ¥æ± ç®¡ç† =================
class AsyncDBPool:
    """å¼‚æ­¥æ•°æ®åº“è¿æ¥æ± ç®¡ç†å™¨"""
    def __init__(self, db_path, pool_size=5):
        self.db_path = db_path
        self.pool_size = pool_size
        self.pool = asyncio.Queue(maxsize=pool_size)
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        if self._initialized:
            return

        # åˆ›å»ºè¡¨ç»“æ„
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute('''CREATE TABLE IF NOT EXISTS watchlist
                              (uid TEXT PRIMARY KEY, exchange_id TEXT, symbol TEXT, high REAL, low REAL, expiry TEXT)''')
            await db.execute('''CREATE TABLE IF NOT EXISTS alert_history
                              (uid TEXT PRIMARY KEY, last_alert TEXT)''')
            await db.commit()

        # é¢„åˆ›å»ºè¿æ¥æ± 
        for _ in range(self.pool_size):
            conn = await aiosqlite.connect(self.db_path)
            await self.pool.put(conn)

        self._initialized = True
        logging.info(f"âœ… æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å®Œæˆ (æ± å¤§å°: {self.pool_size})")

    @asynccontextmanager
    async def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        if not self._initialized:
            await self.initialize()

        conn = await self.pool.get()
        try:
            yield conn
        finally:
            await self.pool.put(conn)

    async def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        while not self.pool.empty():
            conn = await self.pool.get()
            await conn.close()
        self._initialized = False

# å…¨å±€æ•°æ®åº“è¿æ¥æ± 
db_pool = AsyncDBPool(DB_FILE, DB_POOL_SIZE)

# ================= ğŸ’¾ å¼‚æ­¥æ•°æ®åº“æ“ä½œ =================
async def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå·²è¿ç§»åˆ°è¿æ¥æ± ï¼‰"""
    await db_pool.initialize()

async def db_add_watchlist(uid, ex_id, symbol, high, low, expiry):
    """æ·»åŠ /æ›´æ–° watchlist"""
    async with db_pool.get_connection() as db:
        await db.execute(
            "REPLACE INTO watchlist (uid, exchange_id, symbol, high, low, expiry) VALUES (?, ?, ?, ?, ?, ?)",
            (uid, ex_id, symbol, high, low, expiry.isoformat())
        )
        await db.commit()

async def db_remove_watchlist(uid):
    """åˆ é™¤ watchlist"""
    async with db_pool.get_connection() as db:
        await db.execute("DELETE FROM watchlist WHERE uid=?", (uid,))
        await db.commit()

async def db_update_alert(uid, alert_time):
    """æ›´æ–° alert_history"""
    async with db_pool.get_connection() as db:
        await db.execute(
            "REPLACE INTO alert_history (uid, last_alert) VALUES (?, ?)",
            (uid, alert_time.isoformat())
        )
        await db.commit()

async def load_data_from_db():
    """ä»æ•°æ®åº“åŠ è½½ç›‘æ§åˆ—è¡¨å’Œæé†’å†å²"""
    async with db_pool.get_connection() as db:
        # åŠ è½½æé†’å†å²
        async with db.execute("SELECT uid, last_alert FROM alert_history") as cursor:
            async for row in cursor:
                alert_history[row[0]] = datetime.fromisoformat(row[1])

        # åŠ è½½ç›‘æ§åˆ—è¡¨
        restored_count = 0
        now = datetime.now()
        async with db.execute("SELECT uid, exchange_id, symbol, high, low, expiry FROM watchlist") as cursor:
            async for row in cursor:
                uid, ex_id, symbol, high, low, expiry_str = row
                expiry = datetime.fromisoformat(expiry_str)
                if expiry <= now or ex_id not in exchanges_dict:
                    # è¿‡æœŸæˆ–äº¤æ˜“æ‰€ä¸å­˜åœ¨ï¼Œåˆ é™¤
                    await db_remove_watchlist(uid)
                    continue
                watchlist[uid] = {
                    'exchange': exchanges_dict[ex_id], 'symbol': symbol,
                    'high': high, 'low': low, 'expiry': expiry
                }
                restored_count += 1

    logging.info(f"ğŸ”„ æ•°æ®åº“æ¢å¤å®Œæˆ: {restored_count} ä¸ªç›¯ç›˜ä»»åŠ¡.")

# ================= ğŸ“§ é‚®ä»¶ç³»ç»Ÿ (æ”¾å…¥åå°çº¿ç¨‹ï¼Œä¸é˜»å¡ä¸»å¾ªç¯ï¼Œå¸¦é‡è¯•) =================
@async_retry
async def sync_send_email_with_retry(subject, content):
    """å¸¦é‡è¯•æœºåˆ¶çš„åŒæ­¥é‚®ä»¶å‘é€"""
    msg = MIMEText(content, 'plain', 'utf-8')
    msg['From'] = formataddr(["ä¸‡ç‰©å¹¶é›†çŒæ‰‹", SENDER_EMAIL])
    msg['To'] = formataddr(["æŒ‡æŒ¥å®˜", RECEIVER_EMAIL])
    msg['Subject'] = subject

    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
    server.starttls()
    server.login(SENDER_EMAIL, SENDER_PASSWORD)
    server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())
    server.quit()
    logging.info(f"ğŸ“§ é‚®ä»¶å·²å‘é€: {subject}")

async def send_email(subject, content):
    """å¼‚æ­¥åŒ…è£…å™¨ï¼šè®©é‚®ä»¶å‘é€åœ¨ç‹¬ç«‹çº¿ç¨‹æ‰§è¡Œï¼Œç»å¯¹ä¸å¡é¡¿ç›¯ç›˜"""
    try:
        await asyncio.to_thread(sync_send_email_with_retry, subject, content)
    except Exception as e:
        logging.error(f"âŒ é‚®ä»¶å‘é€æœ€ç»ˆå¤±è´¥: {e}")

# ================= ğŸš€ æ ¸å¿ƒå¼‚æ­¥é€»è¾‘ =================

async def init_exchanges():
    logging.info("ğŸ”Œ æ­£åœ¨åˆå§‹åŒ–å¼‚æ­¥äº¤æ˜“æ‰€è”ç›Ÿ...")
    for ex_id, params in EXCHANGES_TO_LOAD.items():
        if hasattr(ccxt, ex_id):
            try:
                ex_class = getattr(ccxt, ex_id)
                exchanges_dict[ex_id] = ex_class(params)
                logging.info(f"âœ… å¼‚æ­¥åŠ è½½æˆåŠŸ: {ex_id.upper()}")
            except Exception as e:
                logging.warning(f"âš ï¸ {ex_id.upper()} åˆå§‹åŒ–å¤±è´¥: {e}")
    if not exchanges_dict:
        logging.error("âŒ æ— å¯ç”¨äº¤æ˜“æ‰€ï¼Œé€€å‡ºï¼")
        exit()

async def get_global_targets():
    logging.info("ğŸ“¡ å¼€å§‹æ‹‰å–å…¨å¸‚åœºæ•°æ®...")
    global_targets = []

    # å¹¶å‘è·å–æ‰€æœ‰äº¤æ˜“æ‰€çš„å¸‚åœºæ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰
    async def fetch_markets(ex_id, ex):
        @async_retry
        async def fetch_with_retry():
            return await ex.load_markets()

        try:
            markets = await fetch_with_retry()
            adaptive_controller.record_request(True)  # è®°å½•æˆåŠŸ
            count = 0
            for symbol, info in markets.items():
                if not (symbol.endswith(':USDT') or symbol.endswith('/USDT')): continue
                if not info.get('active', True): continue
                vol = info.get('quoteVolume', 0)
                if vol and vol > MIN_VOLUME_USDT:
                    global_targets.append({'exchange': ex, 'symbol': symbol, 'vol': vol})
                    count += 1
            logging.info(f"  - {ex_id.upper()}: {count} ä¸ªæ´»è·ƒæ ‡çš„")
        except Exception as e:
            adaptive_controller.record_request(False)  # è®°å½•å¤±è´¥
            logging.warning(f"  - âš ï¸ æ— æ³•æ‹‰å– {ex_id.upper()}: {e}")

    # åŒæ—¶å‘è½¦ï¼
    tasks = [fetch_markets(ex_id, ex) for ex_id, ex in exchanges_dict.items()]
    await asyncio.gather(*tasks)

    global_targets = sorted(global_targets, key=lambda x: x['vol'], reverse=True)
    return global_targets

async def check_structure(ex, symbol):
    """å¹¶å‘å½¢æ€æ£€æµ‹ï¼ˆå¸¦é‡è¯• + è‡ªé€‚åº”å¹¶å‘ï¼‰"""
    async with adaptive_controller.semaphore:  # ä½¿ç”¨è‡ªé€‚åº”å¹¶å‘æ§åˆ¶å™¨
        @async_retry
        async def fetch_with_retry():
            return await ex.fetch_ohlcv(symbol, timeframe='1h', limit=120)

        try:
            bars = await fetch_with_retry()
            adaptive_controller.record_request(True)  # è®°å½•æˆåŠŸ
            if not bars or len(bars) < MIN_HISTORY: return None

            df = pd.DataFrame(bars, columns=['ts', 'open', 'high', 'low', 'close', 'vol'])
            box = df.iloc[-(MIN_HISTORY+1) : -1]

            box_high, box_low = box['high'].max(), box['low'].min()
            amp = (box_high - box_low) / box_low
            if amp > MAX_AMPLITUDE or amp < 0.01: return None

            last_4 = box.iloc[-4:]
            avg_vol = (box['high'] - box['low']).mean()
            recent_vol = (last_4['high'] - last_4['low']).mean()

            if recent_vol > avg_vol * SQUEEZE_FACTOR: return None

            return {'high': box_high, 'low': box_low, 'amp': amp, 'squeeze': recent_vol/avg_vol}
        except Exception as e:
            adaptive_controller.record_request(False)  # è®°å½•å¤±è´¥
            return None

async def radar_job():
    """é›·è¾¾æ‰«æä»»åŠ¡ï¼šæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡"""
    while True:
        try:
            targets = await get_global_targets()
            if targets:
                logging.info(f"ğŸ” å¼€å§‹å¹¶å‘åˆ†æ {len(targets)} ä¸ªæ ‡çš„ K çº¿å½¢æ€...")

                # åˆ›å»ºæ‰€æœ‰ Kçº¿ åˆ†æä»»åŠ¡
                tasks = []
                for target in targets:
                    ex = target['exchange']
                    symbol = target['symbol']
                    uid = f"{ex.id}:{symbol}"
                    if uid not in watchlist:
                        tasks.append(asyncio.create_task(process_single_target(ex, symbol, uid)))

                # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ (ä»¥å‰è¦10åˆ†é’Ÿï¼Œç°åœ¨åªè¦å‡ åç§’ï¼)
                await asyncio.gather(*tasks)
                logging.info("âœ… æœ¬è½®å…¨åŸŸé›·è¾¾å¹¶å‘æ‰«æå®Œæˆã€‚")

                # æ‰“å°è‡ªé€‚åº”å¹¶å‘ç»Ÿè®¡
                stats = adaptive_controller.get_stats()
                logging.info(f"ğŸ“Š å¹¶å‘ç»Ÿè®¡: æˆåŠŸç‡ {stats['success_rate']:.1%}, "
                          f"å½“å‰å¹¶å‘ {stats['current_concurrency']}/{stats['max_concurrency']}, "
                          f"æ€»è¯·æ±‚ {stats['total_requests']}")

        except Exception as e:
            logging.error(f"é›·è¾¾ä»»åŠ¡å¼‚å¸¸: {e}")

        # ç¡ä¸€å°æ—¶ï¼Œå†å»æ‰«
        await asyncio.sleep(RADAR_INTERVAL)

async def process_single_target(ex, symbol, uid):
    """å¤„ç†å•ä¸ªæ ‡çš„çš„å›è°ƒå‡½æ•°"""
    async with analysis_semaphore:  # é™åˆ¶å¹¶å‘åˆ†ææ•°
        struct = await check_structure(ex, symbol)
        if struct:
            expiry_time = datetime.now() + timedelta(hours=WATCHLIST_EXPIRY_HOURS)
            watchlist[uid] = {
                'exchange': ex, 'symbol': symbol, 'expiry': expiry_time,
                'high': struct['high'], 'low': struct['low']
            }
            await db_add_watchlist(uid, ex.id, symbol, struct['high'], struct['low'], expiry_time)

            logging.info(f"ğŸ¯ é”å®šç›®æ ‡: [{ex.id.upper()}] {symbol}")
            # å¼‚æ­¥å‘é‚®ä»¶ï¼Œä¸å¡æµç¨‹
            asyncio.create_task(send_email(
                f"ã€å¹¶é›†å‘ç°ã€‘{symbol} åœ¨ {ex.id.upper()} æ”¶æ•›",
                f"å¹³å°: {ex.id.upper()}\nå“ç§: {symbol}\né˜»åŠ›: {struct['high']}\næ”¯æ’‘: {struct['low']}"
            ))

async def sniper_job():
    """ç‹™å‡»æ‰‹ä»»åŠ¡ï¼šæ­»æ­»ç›¯ä½ Watchlistï¼Œä¸¥æ ¼æ¯ 60 ç§’å¼€ç«ä¸€æ¬¡"""
    while True:
        start_time = time.time()

        if watchlist:
            # logging.info(f"ğŸ”« ç‹™å‡»æ‰‹å·¡è§†ä¸­... å½“å‰ç›®æ ‡æ•°: {len(watchlist)}")
            tasks = []
            for uid in list(watchlist.keys()):
                info = watchlist[uid]
                ex = info['exchange']
                symbol = info['symbol']

                if datetime.now() > info['expiry']:
                    del watchlist[uid]
                    await db_remove_watchlist(uid)
                    logging.info(f"ğŸ—‘ï¸ ç›®æ ‡è¿‡æœŸ: {uid}")
                    continue

                # æ´¾å‘å¹¶å‘ç›¯ç›˜ä»»åŠ¡
                tasks.append(asyncio.create_task(snip_single_target(uid, ex, symbol, info)))

            await asyncio.gather(*tasks)

        # ç²¾ç¡®çš„ 60 ç§’èŠ‚æ‹å™¨è¡¥å¿
        elapsed = time.time() - start_time
        sleep_time = max(SNIPER_INTERVAL - elapsed, 1) # è‡³å°‘ç¡ 1 ç§’é˜²æ­»å¾ªç¯
        await asyncio.sleep(sleep_time)

async def snip_single_target(uid, ex, symbol, info):
    """å¹¶å‘æŸ¥è¯¢æœ€æ–°ä»·ï¼Œåˆ¤æ–­çªç ´ï¼ˆå¸¦é‡è¯•ï¼‰"""
    async with adaptive_controller.semaphore:
        @async_retry
        async def fetch_ticker_with_retry():
            return await ex.fetch_ticker(symbol)

        try:
            # è·å–æœ€æ–°ä»· (èµ°ç½‘ç»œè¯·æ±‚ï¼Œä½†è¢« async æŒ‚èµ·ï¼Œä¸ä¼šé˜»å¡åˆ«äºº)
            ticker = await fetch_ticker_with_retry()
            adaptive_controller.record_request(True)  # è®°å½•æˆåŠŸ
            price = ticker['last']

            signal, break_price = None, None
            if price > info['high']: signal, break_price = "ğŸ“ˆ å‘ä¸Šçªç ´", info['high']
            elif price < info['low']: signal, break_price = "ğŸ“‰ å‘ä¸‹è·Œç ´", info['low']

            if signal:
                logging.warning(f"ğŸš€ ã€å‡»æ€ç¡®è®¤ã€‘ {ex.id.upper()} çš„ {symbol} è§¦å‘çªç ´ï¼ç°ä»·: {price}")

                if uid not in alert_history or datetime.now() - alert_history[uid] > timedelta(hours=ALERT_COOLDOWN_HOURS):
                    email_content = f"ğŸš¨ ã€å¹¶é›†çªç ´è­¦æŠ¥ã€‘{symbol}\n\nå¹³å°: {ex.id.upper()}\næ–¹å‘: {signal}\nç°ä»·: {price}\nçªç ´ä½: {break_price}\n\nå¿«å»æŸ¥çœ‹ï¼"

                    # å¼‚æ­¥å‘é‚®ä»¶
                    asyncio.create_task(send_email(f"ğŸš¨ {signal} {symbol} ({ex.id.upper()})", email_content))

                    alert_history[uid] = datetime.now()
                    await db_update_alert(uid, datetime.now())

                    del watchlist[uid]
                    await db_remove_watchlist(uid)
        except Exception as e:
            adaptive_controller.record_request(False)  # è®°å½•å¤±è´¥
            pass # ç½‘ç»œæŠ–åŠ¨ï¼Œå¿½ç•¥ï¼Œç­‰ä¸‹ä¸€ä¸ª 60 ç§’

async def main():
    print("===================================================")
    print("ğŸš€ ä¸‡ç‰©å¹¶é›†çŒæ‰‹ V15.0 (è‡ªé€‚åº”å¹¶å‘ç‰ˆ) å¯åŠ¨...")
    print("===================================================")

    await init_db()
    await adaptive_controller.initialize()
    await init_exchanges()
    await load_data_from_db()

    # ğŸ”¥ è§è¯å¥‡è¿¹çš„æ—¶åˆ»ï¼šé›·è¾¾å’Œç‹™å‡»æ‰‹ä½œä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å¹¶å‘åç¨‹åŒæ—¶è¿è¡Œï¼
    # äº’ä¸å¹²æ‰°ï¼Œé›·è¾¾æ‰«å¾—å†æ…¢ï¼Œç‹™å‡»æ‰‹ä¹Ÿä¼šå‡†æ—¶åœ¨ç¬¬ 60 ç§’å¼€æªã€‚
    await asyncio.gather(
        radar_job(),
        sniper_job()
    )

    # æ¸…ç†è¿æ¥æ± 
    await db_pool.close_all()

if __name__ == "__main__":
    try:
        # å¯åŠ¨ Python å¼‚æ­¥äº‹ä»¶å¾ªç¯
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("ğŸ›‘ æ¥æ”¶åˆ°é€€å‡ºæŒ‡ä»¤ï¼Œç¨‹åºå®‰å…¨ç»ˆæ­¢ã€‚")
